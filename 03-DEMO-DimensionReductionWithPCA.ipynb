{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction with PCA\n",
    "\n",
    "In this notebook, we will take a look at how PCA can be used to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "To use some realistic data, let's load the famous MNIST dataset, consisting of hand drawn digits $0,1,\\ldots,9$. There is a built in `sklearn` function to load this dataset (it loads a small version of the full MNIST dataset, with a relatively small number of low-resolution images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of MNIST contains 1797 samples of handwritten digits, represented $8 \\times 8$ images. We can think of these as vectors in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$.\n",
    "\n",
    "Here are a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(digits.images[j], cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get nicer-looking pictures by applying a filter to smooth the data (this is only for visualization purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(digits.images[j], cmap='gray', interpolation = 'lanczos')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on MNIST\n",
    "\n",
    "Recall that our `digits.data` matrix has shape 1797 x 64 = samples x features. So this is a \"data matrix\" in the sense we just described in lecture. We can apply PCA to this data matrix -- remember this is just computing the singular vectors of the matrix! There is a built-in `sklearn` function to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA() # Initialize the model\n",
    "pca.fit(digits.data) # fit the model to our data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted PCA model has various attributes. The singularvectors we are interested are stored in the `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.components_.shape)\n",
    "print(pca.components_[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, `pca.components_` is a 64x64 matrix containing all singular vectors of our data matrix as its columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the motivation for PCA was to find a good low-dimensional subspace to project our data onto. Our theory says that the best possible $k$-dimensional subspace (in terms of distorting the data as little as possible) is the one spanned by the first $k$ singular vectors. \n",
    "\n",
    "Let's project onto the first 2 singular vectors. We'll do this 'by hand', so that we can see what's going on. There is built in `sklearn` functionality for this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0 = pca.components_[0]\n",
    "pVec1 = pca.components_[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "projectedMNIST = []\n",
    "\n",
    "# Now we linearly project every sample in our dataset onto the two-dimensional subspace spanned by the first \n",
    "# two singular vectors\n",
    "for j in range(1797):\n",
    "    projectedMNIST.append([np.dot(digits.data[j],pVec0),np.dot(digits.data[j],pVec1)])\n",
    "\n",
    "projectedMNIST = np.array(projectedMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we projected onto a two-dimensional plane, we can now visualize our data!\n",
    "\n",
    "The figure below shows our 64-dimensional data, projected onto our 2-dimensional PCA subspace. The dots are colored by their 'class' (i.e., which digit is drawn, 0,1,...,9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(projectedMNIST[:, 0], projectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('pVec0')\n",
    "plt.ylabel('pVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data is separated out in some sensible way according to class!\n",
    "\n",
    "As a **baseline**, here's what we get if we project onto a random 2-dimensional subspace of our 64-dimensional data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orth\n",
    "\n",
    "RandVecs = orth(np.random.rand(64,2))\n",
    "\n",
    "RandVec0 = RandVecs[:,0]\n",
    "RandVec1 = RandVecs[:,1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "RandProjectedMNIST = []\n",
    "\n",
    "for j in range(1797):\n",
    "    RandProjectedMNIST.append([np.dot(digits.data[j],RandVec0),np.dot(digits.data[j],RandVec1)])\n",
    "\n",
    "RandProjectedMNIST = np.array(RandProjectedMNIST)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(RandProjectedMNIST[:, 0], RandProjectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('RandVec0')\n",
    "plt.ylabel('RandVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that PCA is really doing some work for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image application, we can also visualize our principal vectors--we just need to reshape the 64-dimensional vectors into 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(pVec0.reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('First Principal Vector')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pVec1.reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('Second Principal Vector')\n",
    "\n",
    "k = 60\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pca.components_[k].reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('Principal Vector '+str(k+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing (so we are perhaps subject to confirmation bias...), we see that hte first principal vector seems sensitive to picking out shapes that look like '4', while the second is sensitive to shapes that look like '0'. This is confirmed by the scatter plot. \n",
    "\n",
    "Moreover, looking at the negative PC1 direction, it seems that '3' is the \"least '4'-like\" digit. The PC2 direction shows that '1' is the \"least '0'-like\" digit. \n",
    "\n",
    "As we increas $k$, the $k$th principal vector becomes less and less interpretable to human eyes.\n",
    "\n",
    "Typically, the first few principal vectors capture the majority of the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement above can be quantified using the *explained variance ratio*, which is the function\n",
    "\\begin{align*}\n",
    "\\mathrm{EVR}:\\{1,\\ldots,m\\} &\\to \\mathbb{R} \\\\\n",
    "k &\\mapsto \\frac{1}{T} \\sum_{i=1}^k \\sigma_i^2,\n",
    "\\end{align*}\n",
    "where $\\sigma_i$ is the $i$-th singular value of the data matrix, $m$ is the number of samples, and $T$ is the total sum of squared singular values (to normalize).\n",
    "\n",
    "For our data, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the first few principal vectors are really doing the heavy lifting in representing the data.\n",
    "\n",
    "It is also informative to look at the 'running total' of explained variance, which shows how much total variance is explained as a percentage by the first $k$ principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 80% of the variation in the data is captured by ~10 principal vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimension Reduction Techniques\n",
    "\n",
    "The process of taking data that lives in a high-dimensional space and trying to 'faithfully' represent it in a low-dimensional space is called *dimension reduction*. PCA is the most basic and arguably most easily interpretable dimension reduction algorithm. It is simple because it is all based on orthogonal projection, which is inherently a *linear* operation.\n",
    "\n",
    "On the other hand, there are many other dimension reduction techniques and most of the popular ones are *non-linear*. I.e., they are *not* realized by a linear function $\\Phi:\\mathbb{R}^N \\to \\mathbb{R}^n$ (where $N$ is a large number and $n=2$ or $3$, typically). \n",
    "\n",
    "One such example is called *t-SNE*. We apply this algorithm to MNIST below without explaining how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's time the computation\n",
    "import time \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start = time.time()\n",
    "digits_projected = TSNE(n_components=2).fit_transform(digits.data)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Compute time {np.round(end-start,3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(digits_projected[:, 0], digits_projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we see that the nonlinear dimension reduction performed by t-SNE does *extremely well* at separating the data from different classes.\n",
    "\n",
    "The computation of t-SNE is an optimization problem, which is solved by gradient descent. To make sense of these terms, we need to study *Vector Calculus*, which forms the second part of the course!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
